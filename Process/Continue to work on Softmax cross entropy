there are some differences betweent the python 2.7 and python 3.+ and the errors occur at the labels and logits of the sparse softmax_cross_entropy.

The logits should be the output of the netural network and the size of this item should be [batch_size,num_classes] if with batch.
The size of labels is similar with that of logits.

here is another example to show how this thing works.



    import tensorflow as tf  
      
    #our NN's output  
    logits=tf.constant([[1.0,2.0,3.0],[1.0,2.0,3.0],[1.0,2.0,3.0]])  
    #step1:do softmax  
    y=tf.nn.softmax(logits)  
    #true label  
    y_=tf.constant([[0.0,0.0,1.0],[0.0,0.0,1.0],[0.0,0.0,1.0]])  
    #step2:do cross_entropy  
    cross_entropy = -tf.reduce_sum(y_*tf.log(y))  
    #do cross_entropy just one step  
    cross_entropy2=tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(logits, y_))#dont forget tf.reduce_sum()!!  
 
When I fllow the above codes, the errors occur at the  "cross_entropy2=tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(logits, y_))"

the error is  ValueError: Only call `softmax_cross_entropy_with_logits` with named arguments (labels=..., logits=..., ...), which is casused by the update of the python. In order to solve this problem, I modify this command to 
"cross_entropy2=tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(logits=logits ,labels= y_))"

After that , continue to plug the below codes to print the output values.

with tf.Session() as sess:  
    softmax=sess.run(y)  
    c_e = sess.run(cross_entropy)  
    c_e2 = sess.run(cross_entropy2)  
    print("step1:softmax result=")  
    print(softmax)  
    print("step2:cross_entropy result=")  
    print(c_e)  
    print("Function(softmax_cross_entropy_with_logits) result=")  
    print(c_e2)  
    
    
The ouputs are

        step1:softmax result=  
    [[ 0.09003057  0.24472848  0.66524094]  
     [ 0.09003057  0.24472848  0.66524094]  
     [ 0.09003057  0.24472848  0.66524094]]  
    step2:cross_entropy result=  
    1.22282  
    Function(softmax_cross_entropy_with_logits) result=  
    1.2228  
    

This example solves my recent problem and I am continuing to try to run the sample.
